{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "How to use BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Eto31uQesT8L",
        "dN44-Dzztrc2",
        "wgWt2vQlvqFF"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XzRGghfsM-Y",
        "colab_type": "text"
      },
      "source": [
        "This will summarize the different ways to use BERT that we encountered during the course."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eto31uQesT8L",
        "colab_type": "text"
      },
      "source": [
        "# Tokenization with BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6PichOFsVvJ",
        "colab_type": "text"
      },
      "source": [
        "During any text data preprocessing, there is a tokenization phase involed. The tokenizer used by Google and available with their package is very powerful!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dN44-Dzztrc2",
        "colab_type": "text"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwAY7WMosIKq",
        "colab_type": "code",
        "outputId": "4159110d-475a-4252-c1ac-cb2f666430d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "!pip install bert-for-tf2\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-for-tf2\n",
            "  Downloading https://files.pythonhosted.org/packages/c2/d8/14e0cfa03bbeb72c314f0648267c490bcceec5e8fb25081ec31307b5509c/bert-for-tf2-0.12.6.tar.gz\n",
            "Collecting py-params>=0.7.3\n",
            "  Downloading https://files.pythonhosted.org/packages/ec/17/71c5f3c0ab511de96059358bcc5e00891a804cd4049021e5fa80540f201a/py-params-0.8.2.tar.gz\n",
            "Collecting params-flow>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/0d/12/2604f88932f285a473015a5adabf08496d88dad0f9c1228fab1547ccc9b5/params-flow-0.7.4.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.7.1->bert-for-tf2) (1.17.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.7.1->bert-for-tf2) (4.28.1)\n",
            "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.12.6-cp36-none-any.whl size=29115 sha256=6b3150da5cfc2503b95442af9f1479b9a5a1981a2fab967a2fc1caa245c73359\n",
            "  Stored in directory: /root/.cache/pip/wheels/24/19/54/51eeca468b219a1bc910c54aff87f0648b28a1fb71c115ba0f\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.8.2-cp36-none-any.whl size=4633 sha256=750a3bebd058df6108c2cda801bb170919f45b7155e314a6fbb9ea1ec5643c53\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/3a/9c/baf35d6f17f0c2c6b61bf8ac3ab9fc12df0e41432ccaeecacb\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.7.4-cp36-none-any.whl size=16196 sha256=ba0673c9ec13a1f591644f5f13332fd98bf933325db5da20d5a75c4138f58d44\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/30/40/507b60d68b67ac87f35e95c98f5b296a32f146d5ae1d1d5aa7\n",
            "Successfully built bert-for-tf2 py-params params-flow\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.12.6 params-flow-0.7.4 py-params-0.8.2\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.85\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAqsSOPitx-q",
        "colab_type": "code",
        "outputId": "8e975e9f-f64b-44bc-f61a-30f05f1a4e4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import bert"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZanQROHTt0Ea",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KIzrNQKuAdd",
        "colab_type": "text"
      },
      "source": [
        "Create the tokenizer with the BERT layer we need to call."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wo8nlVYrt2JT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "                            trainable=False) # trainable=False because we won't train this layer, we just need info from it\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cov0VHmYuE_-",
        "colab_type": "text"
      },
      "source": [
        "Applying the tokenizer then converting into ids."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hx7iHTjjt_DX",
        "colab_type": "code",
        "outputId": "553ff5fb-7157-4c29-cbff-202ff699aaff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(tokenizer.tokenize(\"Roses are red, violets are blue.\"))\n",
        "print(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"Roses are red, violets are blue.\")))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['roses', 'are', 'red', ',', 'violet', '##s', 'are', 'blue', '.']\n",
            "[10529, 2024, 2417, 1010, 8766, 2015, 2024, 2630, 1012]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgWt2vQlvqFF",
        "colab_type": "text"
      },
      "source": [
        "# Embedding with BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyrWh6m2vvCu",
        "colab_type": "text"
      },
      "source": [
        "This time we go even further, keeping any NLP model we already have but improving the embedding layer with BERT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u-fRmrCv6Q5",
        "colab_type": "text"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG3Nc6o_vsFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install bert-for-tf2\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1WtRgPKwDzY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import bert"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cr-N6ft85ZoN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from collections import namedtuple"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdbYpd9O4KEj",
        "colab_type": "text"
      },
      "source": [
        "## Inputs creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQgozHFm4MJi",
        "colab_type": "text"
      },
      "source": [
        "Our BERT embedding layer will need three types of input tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkI4HnNC41QZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "                            trainable=False) # trainable=False because we won't train this layer, we just need info from it\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OByOEDY4JW9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_ids(tokens):\n",
        "    return tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "def get_mask(tokens):\n",
        "    return np.char.not_equal(tokens, \"[PAD]\").astype(int)\n",
        "\n",
        "def get_segments(tokens):\n",
        "    seg_ids = []\n",
        "    current_seg_id = 0\n",
        "    for tok in tokens:\n",
        "        seg_ids.append(current_seg_id)\n",
        "        if tok == \"[SEP]\":\n",
        "            current_seg_id = 1-current_seg_id # turns 1 into 0 and vice versa\n",
        "    return seg_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hh_oRwjJ4q63",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_sent = \"Roses are red, violets are blue.\"\n",
        "my_tok_sent = tokenizer.tokenize(my_sent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5J469_i15AeN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_input = tf.expand_dims([get_ids(my_tok_sent),\n",
        "                           get_mask(my_tok_sent),\n",
        "                           get_segments(my_tok_sent)],\n",
        "                          axis=0) # expand_dims to simulate batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Up5Nh2dY63li",
        "colab_type": "code",
        "outputId": "074269de-34dd-41c6-935d-9e4aae880a69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "my_input[:, 0, :]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 9), dtype=int32, numpy=\n",
              "array([[10529,  2024,  2417,  1010,  8766,  2015,  2024,  2630,  1012]],\n",
              "      dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbwAJjmqwFPk",
        "colab_type": "text"
      },
      "source": [
        "## Your model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNcRJKF_wG4e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def YourModel(tf.keras.Model):\n",
        "\n",
        "    def __init__(self,\n",
        "                 # ...,\n",
        "                 ):\n",
        "        super(YourModel, self).__init__()\n",
        "\n",
        "        self.bert_embedder = hub.keras_layer(\n",
        "            \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "            trainable=False) # we still don't want to train this layer, the learning\n",
        "                             # will come with the rest of the model, this is as frozen embedding layer\n",
        "            \n",
        "        # ...\n",
        "\n",
        "    # ...\n",
        "\n",
        "    def call(self, inputs):\n",
        "        _, embedded = self.bert_embedder([inputs[:, 0, :],\n",
        "                                          inputs[:, 1, :],\n",
        "                                          inputs[:, 2, :])\n",
        "        \n",
        "        # ...\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jNR8qTt9lME",
        "colab_type": "text"
      },
      "source": [
        "# Fine-tuning BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ9qK6HWABig",
        "colab_type": "text"
      },
      "source": [
        "This time we use BERT as the core of our model and we fine-tune it.\n",
        "We need to identify to things:\n",
        "\n",
        "*   Which of two outputs from BERT we will use, the first one (sentence-level reprensentation, for calssification for example) or the second one (token-level representation).\n",
        "*   How to use the dense layer we use after BERT that suits our task (a simple dense layer with `nb_units=nb_classes` for a classification for instance)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZHTFljqALje",
        "colab_type": "text"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "953R64LzANwh",
        "colab_type": "text"
      },
      "source": [
        "We will use a different package because it comes with an better optimizer to train BERT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46RORW8i9n1v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tf-models-official\n",
        "!pip install tf-nightly"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJ99qESMAdo6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1kTgXL1AkQD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from official.nlp.bert.tokenization import FullTokenizer\n",
        "from official.nlp import optimization"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxSFmmiUArSV",
        "colab_type": "text"
      },
      "source": [
        "## Inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DUjrLqnAsuW",
        "colab_type": "text"
      },
      "source": [
        "Same process as for the BERT embedding, but we need to add [CLS] and [SEP] tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZXausf8AsW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "                            trainable=False) # trainable=False because we won't train this layer, we just need info from it\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYsT0IUkBQLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_ids(tokens):\n",
        "    return tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "def get_mask(tokens):\n",
        "    return np.char.not_equal(tokens, \"[PAD]\").astype(int)\n",
        "\n",
        "def get_segments(tokens):\n",
        "    seg_ids = []\n",
        "    current_seg_id = 0\n",
        "    for tok in tokens:\n",
        "        seg_ids.append(current_seg_id)\n",
        "        if tok == \"[SEP]\":\n",
        "            current_seg_id = 1-current_seg_id # turns 1 into 0 and vice versa\n",
        "    return seg_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52_60cAGBSPR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_sent_1 = \"Roses are red.\"\n",
        "my_sent_2 = \"Violets are blue.\"\n",
        "my_tok_sent = [\"[CLS]\"] + tokenizer.tokenize(my_sent_1) + [\"[SEP]\"] + tokenizer.tokenize(my_sent_2) + [\"[SEP]\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqgDx_vqJI9a",
        "colab_type": "code",
        "outputId": "44931c41-eb62-4ffa-af8c-6fe206608b84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "my_tok_sent"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'roses',\n",
              " 'are',\n",
              " 'red',\n",
              " '.',\n",
              " '[SEP]',\n",
              " 'violet',\n",
              " '##s',\n",
              " 'are',\n",
              " 'blue',\n",
              " '.',\n",
              " '[SEP]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7B25vB7MBT7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_input = tf.expand_dims([get_ids(my_tok_sent),\n",
        "                           get_mask(my_tok_sent),\n",
        "                           get_segments(my_tok_sent)],\n",
        "                          axis=0) # expand_dims to simulate batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbkLfD5PBVdg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_input[:, 0, :]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvCu6CFNBV-T",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rWLyWD8BaZV",
        "colab_type": "text"
      },
      "source": [
        "We define a simple dense-based layer the will be added after BERT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VykEo-QZBW_J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def MyLayer(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self,\n",
        "                 nb_units):\n",
        "        super(MyLayer, self).__init__()\n",
        "\n",
        "        self.my_dense = tf.keras.layers.Dense(\n",
        "            nb_units,\n",
        "            kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02)) # good initializer for BERT's dense layer\n",
        "    \n",
        "    def cal(self, inputs):\n",
        "        x = self.my_dens(inputs)\n",
        "        \n",
        "        # ... any other task specific computation.\n",
        "        # For classification for instance, this would be enough.\n",
        "\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKLL_uR6Cob8",
        "colab_type": "text"
      },
      "source": [
        "Building our whole model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlrz3G9RCnvs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BERTModel(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 nb_units,\n",
        "                 dropout_rate):\n",
        "        super(BERTModel, self).__init__()\n",
        "\n",
        "        self.dropout_rate = dropout_rate\n",
        "        \n",
        "        self.bert_layer = hub.KerasLayer(\n",
        "            \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "            trainable=True) # Trainable=True to tune the weights for our task!\n",
        "        \n",
        "        self.my_layer = MyLayer(nb_units)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # To use first BERT output (sentence-level representation),\n",
        "        # for classification for instance.\n",
        "\n",
        "        x, _ = self.bert_embedder([inputs[:, 0, :],\n",
        "                                   inputs[:, 1, :],\n",
        "                                   inputs[:, 2, :])\n",
        "        \n",
        "\n",
        "        # To use second BERT output (token-level representation).\n",
        "\n",
        "        _, x = self.bert_embedder([inputs[:, 0, :],\n",
        "                                   inputs[:, 1, :],\n",
        "                                   inputs[:, 2, :])\n",
        "        \n",
        "        x = tf.nn.dropout(x, self.dropout_rate) # Might be good to add a dropout here.\n",
        "\n",
        "        my_output = self.my_layer(x)\n",
        "        \n",
        "        return my_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bH9_dL04Dt31",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X96J0besEcHD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NB_UNITS = 2\n",
        "\n",
        "DROPOUT_RATE = 0.1\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "NB_EPOCHS = 5\n",
        "INIT_LR = 5e-5\n",
        "WARMUP_STEPS = int(NB_BATCHES_TRAIN * 0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEiKzqPiFApj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_model = BERTModel(NB_UNITS,\n",
        "                     DROPOUT_RATE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-Yas3q4DvPk",
        "colab_type": "text"
      },
      "source": [
        "We can use the optimizer provided by the package (a modified Adam)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2jdQFl1Duup",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optimization.create_optimizer(\n",
        "    init_lr=INIT_LR,\n",
        "    num_train_steps=NB_BATCHES_TRAIN,\n",
        "    num_warmup_steps=WARMUP_STEPS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RrStl-_Faiq",
        "colab_type": "text"
      },
      "source": [
        "Let's compile."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqPwx540FS2q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_classifier.compile(optimizer,\n",
        "                        my_loss_fn,\n",
        "                        [my_metric])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPfQKcSPFhZG",
        "colab_type": "text"
      },
      "source": [
        "And now we can fit, evaluate and use our model like any other!"
      ]
    }
  ]
}